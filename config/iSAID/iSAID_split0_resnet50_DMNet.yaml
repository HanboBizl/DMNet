Data:
  data_root:  /workspace/DMNet/data/iSAID
  base_data_root: ./data/base_annotation/pascal/
  train_list: ./lists/iSAID/train.txt
  val_list: ./lists/iSAID/val.txt
  classes: 2


Train:
  # Aug
  train_h: 256
  train_w: 256
  val_size: 256
  scale_min: 0.9  # minimum random scale
  scale_max: 1.1 # maximum random scale
  rotate_min: -10  # minimum random rotate
  rotate_max: 10  # maximum random rotate
  ignore_label: 255
  padding_label: 255
  ppm_scales: [1.0]
  # Dataset & Mode
  split: 0
  shot: 1
  data_set: 'iSAID'
  use_split_coco: False # True means FWB setting
  # Optimizer
  batch_size: 8 # batch size for training (bs8 for 1GPU)
  base_lr: 0.005
  epochs: 200
  start_epoch: 0
  stop_interval: 75 # stop when the best result is not updated for "stop_interval" epochs
  index_split: -1 # index for determining the params group with 10x learning rate
  power: 0.9 # 0 means no decay
  momentum: 0.9
  weight_decay: 0.0001
  warmup: False
  # Viz & Save & Resume
  print_freq: 10
  save_freq: 50
  resume:
  # Validate
  evaluate: True
  SubEpoch_val: True # val at the half epoch
  fix_random_seed_val: True
  batch_size_val: 1
  resized_val: True
  ori_resize: True  # use original label for evaluation
  # Else
  workers: 8
  fix_bn: True
  manual_seed: 321
  seed_deterministic: False
  zoom_factor: 8  # zoom factor for final prediction during training, be in [1, 2, 4, 8]

Method:
  layers: 50  # backbone layers
  vgg: False  #choose backbone
  aux_weight: 1.0 # aux_loss weight
  thresh_fp: 0.7  # CSRM parameter of Foraground
  thresh_bp: 0.6  # CSRM parameter of Foraground
  fuse_select: True # CSRM parameter
  start_loss: 0.8   # KMS start

# for testing
Test_Finetune:
  weight: "/workspace/exp/res50_kshot/split0/res50_12_10_copy/snapshot/train5_epoch_72_0.5767.pth"  # checkpoint
  proto_dict_path: "/workspace/exp/res50_kshot/split2/res50_12_10_copy/snapshotproto_dict.pth" # the path of the KMS dict (only FP for testing)
  ann_type: 'mask' # mask/bbox


## deprecated multi-processing training
# Distributed:
#   dist_url: tcp://127.0.0.1:6789
#   dist_backend: 'nccl'
#   multiprocessing_distributed: False
#   world_size: 1
#   rank: 0
#   use_apex: False
#   opt_level: 'O0'
#   keep_batchnorm_fp32:
#   loss_scale:

